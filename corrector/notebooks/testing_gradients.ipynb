{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "11bbe946",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"6\"\n",
    "\n",
    "from corrector_src.model._cnn_mhd_corrector import CorrectorCNN\n",
    "import jax.numpy as jnp\n",
    "import jax\n",
    "import equinox as eqx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "91aef0b9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output shape: (8, 32, 32, 32)\n",
      "Output norm: nan\n",
      "Gradient norms: CorrectorCNN(\n",
      "  layers=Sequential(\n",
      "    layers=(\n",
      "      Conv3d(\n",
      "        num_spatial_dims=3,\n",
      "        weight=f32[],\n",
      "        bias=f32[],\n",
      "        in_channels=8,\n",
      "        out_channels=16,\n",
      "        kernel_size=(3, 3, 3),\n",
      "        stride=(1, 1, 1),\n",
      "        padding=((1, 1), (1, 1), (1, 1)),\n",
      "        dilation=(1, 1, 1),\n",
      "        groups=1,\n",
      "        use_bias=True,\n",
      "        padding_mode='ZEROS'\n",
      "      ),\n",
      "      Lambda(fn=None),\n",
      "      Conv3d(\n",
      "        num_spatial_dims=3,\n",
      "        weight=f32[],\n",
      "        bias=f32[],\n",
      "        in_channels=16,\n",
      "        out_channels=16,\n",
      "        kernel_size=(3, 3, 3),\n",
      "        stride=(1, 1, 1),\n",
      "        padding=((1, 1), (1, 1), (1, 1)),\n",
      "        dilation=(1, 1, 1),\n",
      "        groups=1,\n",
      "        use_bias=True,\n",
      "        padding_mode='ZEROS'\n",
      "      ),\n",
      "      Lambda(fn=None),\n",
      "      Conv3d(\n",
      "        num_spatial_dims=3,\n",
      "        weight=f32[],\n",
      "        bias=None,\n",
      "        in_channels=16,\n",
      "        out_channels=8,\n",
      "        kernel_size=(3, 3, 3),\n",
      "        stride=(1, 1, 1),\n",
      "        padding=((1, 1), (1, 1), (1, 1)),\n",
      "        dilation=(1, 1, 1),\n",
      "        groups=1,\n",
      "        use_bias=False,\n",
      "        padding_mode='ZEROS'\n",
      "      )\n",
      "    )\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "# Create a test model\n",
    "key = jax.random.PRNGKey(42)\n",
    "test_model = CorrectorCNN(in_channels=8, hidden_channels=16, key=key)\n",
    "test_model = eqx.tree_deserialise_leaves(\n",
    "    \"/export/home/jalegria/Thesis/jf1uids/experiments/experiment_1/2025-10-01_15-47-48_10/cnn_model.eqx\",\n",
    "    test_model,\n",
    ")\n",
    "\n",
    "# Test input\n",
    "test_input = jnp.ones((8, 32, 32, 32))\n",
    "\n",
    "# Test forward pass\n",
    "output = test_model(test_input)\n",
    "print(f\"Output shape: {output.shape}\")\n",
    "print(f\"Output norm: {jnp.linalg.norm(output)}\")\n",
    "\n",
    "# Test gradients\n",
    "def simple_loss(model, x):\n",
    "    return jnp.mean(model(x)**2)\n",
    "\n",
    "grads = eqx.filter_grad(simple_loss)(test_model, test_input)\n",
    "print(f\"Gradient norms: {jax.tree.map(lambda x: jnp.linalg.norm(x) if hasattr(x, 'shape') else 0, grads)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ad68638e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient norms by layer:\n",
      "  Layer 0 - Weight: 0.000000, Bias: 0.000000\n",
      "  Layer 1 - No gradients (activation layer)\n",
      "  Layer 2 - Weight: nan, Bias: 0.000000\n",
      "  Layer 3 - No gradients (activation layer)\n",
      "  Layer 4 - Weight: nan, Bias: 0.000000\n"
     ]
    }
   ],
   "source": [
    "def print_gradient_norms(grads):\n",
    "    \"\"\"Properly print gradient norms for each layer\"\"\"\n",
    "    def get_norm(x):\n",
    "        if hasattr(x, 'shape') and x.size > 0:\n",
    "            return float(jnp.linalg.norm(x))\n",
    "        return 0.0\n",
    "    \n",
    "    print(\"Gradient norms by layer:\")\n",
    "    for i, layer in enumerate(grads.layers.layers):\n",
    "        if hasattr(layer, 'weight') and layer.weight is not None:\n",
    "            weight_norm = get_norm(layer.weight)\n",
    "            bias_norm = get_norm(layer.bias) if layer.bias is not None else 0.0\n",
    "            print(f\"  Layer {i} - Weight: {weight_norm:.6f}, Bias: {bias_norm:.6f}\")\n",
    "        else:\n",
    "            print(f\"  Layer {i} - No gradients (activation layer)\")\n",
    "\n",
    "# Test with proper printing\n",
    "grads = eqx.filter_grad(simple_loss)(test_model, test_input)\n",
    "print_gradient_norms(grads)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1f4b388b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from corrector_src.training.training_config import TrainingConfig\n",
    "from corrector_src.model._cnn_mhd_corrector_options import (\n",
    "    CNNMHDParams,\n",
    "    CNNMHDconfig,\n",
    ")\n",
    "from jf1uids.option_classes.simulation_config import finalize_config\n",
    "from jf1uids import time_integration\n",
    "\n",
    "import corrector_src.data.blast_creation as blast\n",
    "from corrector_src.utils.downaverage import downaverage_states\n",
    "from corrector_src.training.loss import mse_loss\n",
    "from jf1uids._physics_modules._cnn_mhd_corrector._cnn_mhd_corrector import _cnn_mhd_corrector\n",
    "\n",
    "\n",
    "n_look_behind = 10\n",
    "epochs = 1\n",
    "num_cells_hr = 128\n",
    "downsampling_factor = 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "5c615cb4",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'list' object has no attribute 'debug'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[11], line 27\u001b[0m\n\u001b[1;32m     17\u001b[0m epoch_losses \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m     18\u001b[0m randomized_vars \u001b[38;5;241m=\u001b[39m [\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m1\u001b[39m]\n\u001b[1;32m     20\u001b[0m (\n\u001b[1;32m     21\u001b[0m     initial_state,\n\u001b[1;32m     22\u001b[0m     config,\n\u001b[1;32m     23\u001b[0m     params,\n\u001b[1;32m     24\u001b[0m     helper_data,\n\u001b[1;32m     25\u001b[0m     registered_variables,\n\u001b[1;32m     26\u001b[0m     _,\n\u001b[0;32m---> 27\u001b[0m ) \u001b[38;5;241m=\u001b[39m \u001b[43mblast\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrandomized_initial_blast_state\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnum_cells_hr\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrandomized_vars\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     29\u001b[0m config \u001b[38;5;241m=\u001b[39m finalize_config(config, initial_state\u001b[38;5;241m.\u001b[39mshape)\n\u001b[1;32m     31\u001b[0m config \u001b[38;5;241m=\u001b[39m config\u001b[38;5;241m.\u001b[39m_replace(cnn_mhd_corrector_config\u001b[38;5;241m=\u001b[39mcnn_mhd_corrector_config)\n",
      "File \u001b[0;32m~/Thesis/jf1uids/corrector_src/data/blast_creation.py:155\u001b[0m, in \u001b[0;36mrandomized_initial_blast_state\u001b[0;34m(num_cells, cfg_data, rng_seed)\u001b[0m\n\u001b[1;32m    151\u001b[0m mhd \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m    153\u001b[0m \u001b[38;5;66;03m# setup simulation config\u001b[39;00m\n\u001b[1;32m    154\u001b[0m config \u001b[38;5;241m=\u001b[39m SimulationConfig(\n\u001b[0;32m--> 155\u001b[0m     runtime_debugging\u001b[38;5;241m=\u001b[39m\u001b[43mcfg_data\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdebug\u001b[49m,\n\u001b[1;32m    156\u001b[0m     first_order_fallback\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[1;32m    157\u001b[0m     progress_bar\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[1;32m    158\u001b[0m     dimensionality\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m3\u001b[39m,\n\u001b[1;32m    159\u001b[0m     num_ghost_cells\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m,\n\u001b[1;32m    160\u001b[0m     box_size\u001b[38;5;241m=\u001b[39mbox_size,\n\u001b[1;32m    161\u001b[0m     num_cells\u001b[38;5;241m=\u001b[39mnum_cells,\n\u001b[1;32m    162\u001b[0m     mhd\u001b[38;5;241m=\u001b[39mmhd,\n\u001b[1;32m    163\u001b[0m     fixed_timestep\u001b[38;5;241m=\u001b[39mfixed_timestep,\n\u001b[1;32m    164\u001b[0m     differentiation_mode\u001b[38;5;241m=\u001b[39mBACKWARDS,\n\u001b[1;32m    165\u001b[0m     riemann_solver\u001b[38;5;241m=\u001b[39mHLL,\n\u001b[1;32m    166\u001b[0m     limiter\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m,\n\u001b[1;32m    167\u001b[0m     return_snapshots\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[1;32m    168\u001b[0m     num_snapshots\u001b[38;5;241m=\u001b[39mcfg_data\u001b[38;5;241m.\u001b[39mnum_snapshots,\n\u001b[1;32m    169\u001b[0m     boundary_settings\u001b[38;5;241m=\u001b[39mBoundarySettings(),\n\u001b[1;32m    170\u001b[0m     num_checkpoints\u001b[38;5;241m=\u001b[39mcfg_data\u001b[38;5;241m.\u001b[39mnum_checkpoints,\n\u001b[1;32m    171\u001b[0m     \u001b[38;5;66;03m# boundary_settings=BoundarySettings(\u001b[39;00m\n\u001b[1;32m    172\u001b[0m     \u001b[38;5;66;03m#    x=BoundarySettings1D(PERIODIC_BOUNDARY, PERIODIC_BOUNDARY),\u001b[39;00m\n\u001b[1;32m    173\u001b[0m     \u001b[38;5;66;03m#    y=BoundarySettings1D(PERIODIC_BOUNDARY, PERIODIC_BOUNDARY),\u001b[39;00m\n\u001b[1;32m    174\u001b[0m     \u001b[38;5;66;03m#    z=BoundarySettings1D(PERIODIC_BOUNDARY, PERIODIC_BOUNDARY),\u001b[39;00m\n\u001b[1;32m    175\u001b[0m     \u001b[38;5;66;03m# ),\u001b[39;00m\n\u001b[1;32m    176\u001b[0m )\n\u001b[1;32m    178\u001b[0m helper_data \u001b[38;5;241m=\u001b[39m get_helper_data(config)\n\u001b[1;32m    179\u001b[0m registered_variables \u001b[38;5;241m=\u001b[39m get_registered_variables(config)\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'list' object has no attribute 'debug'"
     ]
    }
   ],
   "source": [
    "training_config = TrainingConfig(\n",
    "    compute_intermediate_losses=True,\n",
    "    n_look_behind=n_look_behind,\n",
    "    loss_weights=None,\n",
    "    use_relative_error=False,\n",
    ")\n",
    "\n",
    "neural_net_params, neural_net_static = eqx.partition(test_model, eqx.is_array)\n",
    "\n",
    "cnn_mhd_corrector_config = CNNMHDconfig(\n",
    "    cnn_mhd_corrector=True, network_static=neural_net_static\n",
    ")\n",
    "\n",
    "cnn_mhd_corrector_params = CNNMHDParams(network_params=neural_net_params)\n",
    "\n",
    "snapshot_losses = []\n",
    "epoch_losses = []\n",
    "randomized_vars = [1, 1, 1]\n",
    "\n",
    "(\n",
    "    initial_state,\n",
    "    config,\n",
    "    params,\n",
    "    helper_data,\n",
    "    registered_variables,\n",
    "    _,\n",
    ") = blast.randomized_initial_blast_state(num_cells_hr, randomized_vars)\n",
    "\n",
    "config = finalize_config(config, initial_state.shape)\n",
    "\n",
    "config = config._replace(cnn_mhd_corrector_config=cnn_mhd_corrector_config)\n",
    "params = params._replace(cnn_mhd_corrector_params=cnn_mhd_corrector_params)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "af2919ca",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'neural_net_params' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[7], line 41\u001b[0m\n\u001b[1;32m     37\u001b[0m     print_gradient_norms(grads)\n\u001b[1;32m     39\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m grads\n\u001b[0;32m---> 41\u001b[0m corrector_grads \u001b[38;5;241m=\u001b[39m \u001b[43mtest_corrector_gradients\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[7], line 29\u001b[0m, in \u001b[0;36mtest_corrector_gradients\u001b[0;34m()\u001b[0m\n\u001b[1;32m     26\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m jnp\u001b[38;5;241m.\u001b[39mmean((corrected_state \u001b[38;5;241m-\u001b[39m test_target)\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m2\u001b[39m)\n\u001b[1;32m     28\u001b[0m \u001b[38;5;66;03m# Get trainable params from your actual model\u001b[39;00m\n\u001b[0;32m---> 29\u001b[0m trainable_params, _ \u001b[38;5;241m=\u001b[39m eqx\u001b[38;5;241m.\u001b[39mpartition(\u001b[43mneural_net_params\u001b[49m, eqx\u001b[38;5;241m.\u001b[39mis_array)\n\u001b[1;32m     31\u001b[0m \u001b[38;5;66;03m# Compute gradients\u001b[39;00m\n\u001b[1;32m     32\u001b[0m loss_val \u001b[38;5;241m=\u001b[39m corrector_loss_fn(trainable_params)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'neural_net_params' is not defined"
     ]
    }
   ],
   "source": [
    "def test_corrector_gradients():\n",
    "    \"\"\"Test if the corrector itself produces gradients\"\"\"\n",
    "    \n",
    "    # Create test data matching your actual shapes\n",
    "    test_primitive_state = jnp.ones((8, 34, 34, 34))  # Adjust to your actual padded shape\n",
    "    test_target = test_primitive_state + 0.01 * jax.random.normal(jax.random.PRNGKey(42), test_primitive_state.shape)\n",
    "    \n",
    "    def corrector_loss_fn(network_params):\n",
    "        # Create updated params\n",
    "        updated_params = params._replace(\n",
    "            cnn_mhd_corrector_params=params.cnn_mhd_corrector_params._replace(\n",
    "                network_params=network_params\n",
    "            )\n",
    "        )\n",
    "        \n",
    "        # Run corrector\n",
    "        corrected_state = _cnn_mhd_corrector(\n",
    "            test_primitive_state,\n",
    "            config,\n",
    "            registered_variables,\n",
    "            updated_params,\n",
    "            jnp.array(0.01)  # timestep\n",
    "        )\n",
    "        \n",
    "        # Simple loss\n",
    "        return jnp.mean((corrected_state - test_target)**2)\n",
    "    \n",
    "    # Get trainable params from your actual model\n",
    "    trainable_params, _ = eqx.partition(neural_net_params, eqx.is_array)\n",
    "    \n",
    "    # Compute gradients\n",
    "    loss_val = corrector_loss_fn(trainable_params)\n",
    "    grads = jax.grad(corrector_loss_fn)(trainable_params)\n",
    "    \n",
    "    print(f\"Corrector test - Loss: {loss_val:.6f}\")\n",
    "    print(\"Corrector gradients:\")\n",
    "    print_gradient_norms(grads)\n",
    "    \n",
    "    return grads\n",
    "\n",
    "corrector_grads = test_corrector_gradients()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "072cdbe5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from jf1uids.option_classes.simulation_config import (\n",
    "    BACKWARDS,\n",
    "    CARTESIAN,\n",
    "    FORWARDS,\n",
    "    STATE_TYPE,\n",
    ")\n",
    "from jf1uids.fluid_equations.total_quantities import (\n",
    "    calculate_internal_energy,\n",
    "    calculate_total_mass,\n",
    ")\n",
    "from jf1uids.fluid_equations.total_quantities import (\n",
    "    calculate_total_energy,\n",
    "    calculate_kinetic_energy,\n",
    "    calculate_gravitational_energy,\n",
    ")\n",
    "\n",
    "from jf1uids.data_classes.simulation_snapshot_data import SnapshotData\n",
    "from jf1uids.time_stepping._timestep_estimator import (\n",
    "    _cfl_time_step,\n",
    "    _source_term_aware_time_step,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "87f8c61c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def unpad_state(state):\n",
    "    \"\"\"Helper function to remove ghost cells.\"\"\"\n",
    "    if config.geometry == CARTESIAN:\n",
    "        if config.dimensionality == 1:\n",
    "            return jax.lax.slice_in_dim(state, 2, state.shape[1] - 2, axis=1)\n",
    "        elif config.dimensionality == 2:\n",
    "            unpad_state = jax.lax.slice_in_dim(state, 2, state.shape[1] - 2, axis=1)\n",
    "            return jax.lax.slice_in_dim(\n",
    "                unpad_state, 2, unpad_state.shape[2] - 2, axis=2\n",
    "            )\n",
    "        elif config.dimensionality == 3:\n",
    "            unpad_state = jax.lax.slice_in_dim(state, 2, state.shape[1] - 2, axis=1)\n",
    "            unpad_state = jax.lax.slice_in_dim(\n",
    "                unpad_state, 2, unpad_state.shape[2] - 2, axis=2\n",
    "            )\n",
    "            return jax.lax.slice_in_dim(\n",
    "                unpad_state, 2, unpad_state.shape[3] - 2, axis=3\n",
    "            )\n",
    "    return state\n",
    "\n",
    "\n",
    "def update_simulation_data(time, state, sim_data, step_idx):\n",
    "    \"\"\"Update simulation data at given step.\"\"\"\n",
    "    unpadded_state = unpad_state(state)\n",
    "\n",
    "    time_points = sim_data.time_points.at[step_idx].set(time)\n",
    "    states = sim_data.states.at[step_idx].set(unpadded_state)\n",
    "\n",
    "    total_mass = sim_data.total_mass.at[step_idx].set(\n",
    "        calculate_total_mass(unpadded_state, helper_data, config)\n",
    "    )\n",
    "    total_energy = sim_data.total_energy.at[step_idx].set(\n",
    "        calculate_total_energy(\n",
    "            unpadded_state,\n",
    "            helper_data,\n",
    "            params.gamma,\n",
    "            params.gravitational_constant,\n",
    "            config,\n",
    "            registered_variables,\n",
    "        )\n",
    "    )\n",
    "    internal_energy = sim_data.internal_energy.at[step_idx].set(\n",
    "        calculate_internal_energy(\n",
    "            unpadded_state,\n",
    "            helper_data,\n",
    "            params.gamma,\n",
    "            config,\n",
    "            registered_variables,\n",
    "        )\n",
    "    )\n",
    "    kinetic_energy = sim_data.kinetic_energy.at[step_idx].set(\n",
    "        calculate_kinetic_energy(\n",
    "            unpadded_state, helper_data, config, registered_variables\n",
    "        )\n",
    "    )\n",
    "\n",
    "    if config.self_gravity:\n",
    "        gravitational_energy = sim_data.gravitational_energy.at[step_idx].set(\n",
    "            calculate_gravitational_energy(\n",
    "                unpadded_state,\n",
    "                helper_data,\n",
    "                params.gravitational_constant,\n",
    "                config,\n",
    "                registered_variables,\n",
    "            )\n",
    "        )\n",
    "    else:\n",
    "        gravitational_energy = sim_data.gravitational_energy\n",
    "\n",
    "    current_checkpoint = step_idx + 1\n",
    "\n",
    "    return sim_data._replace(\n",
    "        time_points=time_points,\n",
    "        states=states,\n",
    "        current_checkpoint=current_checkpoint,\n",
    "        total_mass=total_mass,\n",
    "        total_energy=total_energy,\n",
    "        internal_energy=internal_energy,\n",
    "        kinetic_energy=kinetic_energy,\n",
    "        gravitational_energy=gravitational_energy,\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0578047",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Larger target test - Loss: 0.250621\n",
      "Gradients with larger target:\n",
      "Gradient norms by layer:\n",
      "  Layer 0 - Weight: 0.076470, Bias: 0.005137\n",
      "  Layer 1 - No gradients (activation layer)\n",
      "  Layer 2 - Weight: 0.050064, Bias: 0.010012\n",
      "  Layer 3 - No gradients (activation layer)\n",
      "  Layer 4 - Weight: 0.040318, Bias: 0.000398\n"
     ]
    }
   ],
   "source": [
    "test_primitive_state = jnp.ones((8, 34, 34, 34))\n",
    "test_target = test_primitive_state + 0.5 * jax.random.normal(jax.random.PRNGKey(42), test_primitive_state.shape)\n",
    "if config.geometry == CARTESIAN:\n",
    "    original_shape = initial_state.shape\n",
    "\n",
    "    if config.dimensionality == 1:\n",
    "        initial_state = jnp.pad(initial_state, ((0, 0), (2, 2)), mode=\"edge\")\n",
    "    elif config.dimensionality == 2:\n",
    "        initial_state = jnp.pad(\n",
    "            initial_state, ((0, 0), (2, 2), (2, 2)), mode=\"edge\"\n",
    "        )\n",
    "    elif config.dimensionality == 3:\n",
    "        initial_state = jnp.pad(\n",
    "            initial_state, ((0, 0), (2, 2), (2, 2), (2, 2)), mode=\"edge\"\n",
    "        )\n",
    "\n",
    "\n",
    "total_steps = 2\n",
    "full_time_points = jnp.zeros(total_steps)\n",
    "full_states = jnp.zeros((total_steps, *original_shape))\n",
    "full_total_mass = jnp.zeros(total_steps)\n",
    "full_total_energy = jnp.zeros(total_steps)\n",
    "full_internal_energy = jnp.zeros(total_steps)\n",
    "full_kinetic_energy = jnp.zeros(total_steps)\n",
    "\n",
    "if config.self_gravity:\n",
    "    full_gravitational_energy = jnp.zeros(total_steps)\n",
    "else:\n",
    "    full_gravitational_energy = None\n",
    "\n",
    "full_sim_data = SnapshotData(\n",
    "    time_points=full_time_points,\n",
    "    states=full_states,\n",
    "    total_mass=full_total_mass,\n",
    "    total_energy=full_total_energy,\n",
    "    internal_energy=full_internal_energy,\n",
    "    kinetic_energy=full_kinetic_energy,\n",
    "    gravitational_energy=full_gravitational_energy,\n",
    "    current_checkpoint=0,\n",
    ")\n",
    "\n",
    "\n",
    "def test_corrector_with_larger_target(carry):\n",
    "    (\n",
    "        state,\n",
    "        time,\n",
    "        sim_data,\n",
    "        network_params,\n",
    "        lag_data,\n",
    "        opt_state,\n",
    "    ) = carry\n",
    "\n",
    "    def corrector_loss_fn(network_params, carry):\n",
    "        (\n",
    "            state,\n",
    "            time,\n",
    "            sim_data,\n",
    "            network_params,\n",
    "            lag_data,\n",
    "            opt_state,\n",
    "        ) = carry\n",
    "\n",
    "        if not config.fixed_timestep:\n",
    "            if config.source_term_aware_timestep:\n",
    "                dt = jax.lax.stop_gradient(\n",
    "                    _source_term_aware_time_step(\n",
    "                        state,\n",
    "                        config,\n",
    "                        updated_params,\n",
    "                        helper_data,\n",
    "                        registered_variables,\n",
    "                        current_time,\n",
    "                    )\n",
    "                )\n",
    "            else:\n",
    "                dt = jax.lax.stop_gradient(\n",
    "                    _cfl_time_step(\n",
    "                        state,\n",
    "                        config.grid_spacing,\n",
    "                        params.dt_max,\n",
    "                        params.gamma,\n",
    "                        config,\n",
    "                        registered_variables,\n",
    "                        params.C_cfl,\n",
    "                    )\n",
    "                )\n",
    "        else:\n",
    "            dt = jnp.asarray(params.t_end / config.num_timesteps)\n",
    "\n",
    "        updated_params = params._replace(\n",
    "            cnn_mhd_corrector_params=params.cnn_mhd_corrector_params._replace(\n",
    "                network_params=network_params\n",
    "            )\n",
    "        )\n",
    "        \n",
    "        corrected_state = _cnn_mhd_corrector(\n",
    "            test_primitive_state,\n",
    "            config,\n",
    "            registered_variables,\n",
    "            updated_params,\n",
    "            jnp.array(0.1)  # Larger timestep\n",
    "        )\n",
    "        \n",
    "        return jnp.mean((corrected_state - test_target)**2)\n",
    "    \n",
    "    trainable_params, _ = eqx.partition(neural_net_params, eqx.is_array)\n",
    "    \n",
    "    loss_val = corrector_loss_fn(trainable_params, carry)\n",
    "    grads = jax.grad(corrector_loss_fn)(trainable_params)\n",
    "    \n",
    "    print(f\"Larger target test - Loss: {loss_val:.6f}\")\n",
    "    print(\"Gradients with larger target:\")\n",
    "    print_gradient_norms(grads)\n",
    "\n",
    "test_corrector_with_larger_target()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "1ac33e3b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running single training step debug...\n",
      "\n",
      "=== STEP 0 ===\n",
      "Testing parameter sensitivity:\n",
      "  Using masked loss (length 1/3)\n",
      "  State norm: 512.425329\n",
      "  Correction effect: 0.652386\n",
      "  Predicted norm: 512.425329\n",
      "  Target norm: 891.236440\n",
      "  Loss value: 0.00333105\n",
      "  Using masked loss (length 1/3)\n",
      "  Original loss: 0.00333105\n",
      "  Perturbed loss: 0.00333268\n",
      "  Sensitivity: 0.00000163\n",
      "Computing gradients...\n",
      "  Using masked loss (length 1/3)\n",
      "Step 0 Gradient norms:\n",
      "  Layer 0 - Weight: 0.00004431, Bias: 0.00000312\n",
      "  Layer 1 - Activation layer (no gradients)\n",
      "  Layer 2 - Weight: 0.00002060, Bias: 0.00000597\n",
      "  Total gradient norm: 0.00004933\n",
      "  Parameter change norm: 0.05416974\n",
      "\n",
      "Final Summary:\n",
      "Loss: 0.00333271\n",
      "Total gradient norm: 0.00004933\n",
      "\n",
      "=== STEP 1 ===\n",
      "Testing parameter sensitivity:\n",
      "  Using masked loss (length 2/3)\n",
      "  State norm: 512.640266\n",
      "  Correction effect: 0.335387\n",
      "  Predicted norm: 724.830850\n",
      "  Target norm: 891.236440\n",
      "  Loss value: 0.00667915\n",
      "  Using masked loss (length 2/3)\n",
      "  Original loss: 0.00667915\n",
      "  Perturbed loss: 0.00668017\n",
      "  Sensitivity: 0.00000102\n",
      "Computing gradients...\n",
      "  Using masked loss (length 2/3)\n",
      "Step 1 Gradient norms:\n",
      "  Layer 0 - Weight: 0.00003535, Bias: 0.00000247\n",
      "  Layer 1 - Activation layer (no gradients)\n",
      "  Layer 2 - Weight: 0.00001081, Bias: 0.00000610\n",
      "  Total gradient norm: 0.00003754\n",
      "  Parameter change norm: 0.04245381\n",
      "Step 1 - Loss: 0.00668014, Grad norm: 0.00003754\n",
      "\n",
      "=== STEP 2 ===\n",
      "Testing parameter sensitivity:\n",
      "  Using full loss (length 3)\n",
      "  State norm: 512.701180\n",
      "  Correction effect: 0.183732\n",
      "  Predicted norm: 887.830086\n",
      "  Target norm: 891.236440\n",
      "  Loss value: 0.01002758\n",
      "  Using full loss (length 3)\n",
      "  Original loss: 0.01002758\n",
      "  Perturbed loss: 0.01002803\n",
      "  Sensitivity: 0.00000045\n",
      "Computing gradients...\n",
      "  Using full loss (length 3)\n",
      "Step 2 Gradient norms:\n",
      "  Layer 0 - Weight: 0.00002433, Bias: 0.00000170\n",
      "  Layer 1 - Activation layer (no gradients)\n",
      "  Layer 2 - Weight: 0.00000486, Bias: 0.00000580\n",
      "  Total gradient norm: 0.00002554\n",
      "  Parameter change norm: 0.03809406\n",
      "Step 2 - Loss: 0.01002800, Grad norm: 0.00002554\n"
     ]
    }
   ],
   "source": [
    "import jax\n",
    "import jax.numpy as jnp\n",
    "import equinox as eqx\n",
    "import optax\n",
    "\n",
    "# Mock your data structures and functions\n",
    "def mock_simulation_step(state, network_params, step_idx):\n",
    "    \"\"\"Mock version of your simulation step\"\"\"\n",
    "    # Simulate what your physics modules do\n",
    "    correction = network_params(state)  # Direct network call for simplicity\n",
    "    corrected_state = state + 0.01 * correction  # time_step * correction\n",
    "    return corrected_state\n",
    "\n",
    "def mock_loss_function(pred_states, gt_states, training_config):\n",
    "    \"\"\"Mock your loss function\"\"\"\n",
    "    return jnp.mean((pred_states - gt_states)**2)\n",
    "\n",
    "# Create mock data\n",
    "key = jax.random.PRNGKey(42)\n",
    "state_shape = (8, 32, 32, 32)  # Adjust to your actual shape\n",
    "data_lag = 3\n",
    "total_steps = 5\n",
    "\n",
    "# Mock initial state\n",
    "initial_state = jnp.ones(state_shape)\n",
    "\n",
    "# Mock target data (slightly different from what simulation would produce)\n",
    "target_data = jnp.ones((total_steps, *state_shape))\n",
    "for i in range(total_steps):\n",
    "    noise = 0.1 * jax.random.normal(jax.random.split(key)[i], state_shape)\n",
    "    target_data = target_data.at[i].set(initial_state + noise)\n",
    "\n",
    "# Mock network (simplified version of your CorrectorCNN)\n",
    "class MockCorrectorCNN(eqx.Module):\n",
    "    layers: eqx.nn.Sequential\n",
    "    \n",
    "    def __init__(self, in_channels, hidden_channels, *, key):\n",
    "        key1, key2, key3 = jax.random.split(key, 3)\n",
    "        self.layers = eqx.nn.Sequential([\n",
    "            eqx.nn.Conv3d(in_channels, hidden_channels, 3, padding=1, key=key1),\n",
    "            eqx.nn.Lambda(jax.nn.relu),\n",
    "            eqx.nn.Conv3d(hidden_channels, in_channels, 3, padding=1, key=key2),\n",
    "        ])\n",
    "    \n",
    "    def __call__(self, x):\n",
    "        return self.layers(x)\n",
    "\n",
    "# Initialize network and optimizer\n",
    "network = MockCorrectorCNN(8, 16, key=key)\n",
    "trainable_params, static_params = eqx.partition(network, eqx.is_array)\n",
    "optimizer = optax.adam(1e-3)\n",
    "opt_state = optimizer.init(trainable_params)\n",
    "\n",
    "# Storage for simulation data\n",
    "simulation_states = jnp.zeros((total_steps, *state_shape))\n",
    "current_state = initial_state\n",
    "\n",
    "def print_gradient_norms(grads, step_name=\"\"):\n",
    "    \"\"\"Helper to print gradient norms\"\"\"\n",
    "    def get_norm(x):\n",
    "        if hasattr(x, 'shape') and x.size > 0:\n",
    "            return float(jnp.linalg.norm(x))\n",
    "        return 0.0\n",
    "    \n",
    "    print(f\"{step_name} Gradient norms:\")\n",
    "    layer_idx = 0\n",
    "    for layer in grads.layers.layers:\n",
    "        if hasattr(layer, 'weight') and layer.weight is not None:\n",
    "            weight_norm = get_norm(layer.weight)\n",
    "            bias_norm = get_norm(layer.bias) if layer.bias is not None else 0.0\n",
    "            print(f\"  Layer {layer_idx} - Weight: {weight_norm:.8f}, Bias: {bias_norm:.8f}\")\n",
    "            layer_idx += 1\n",
    "        else:\n",
    "            print(f\"  Layer {layer_idx} - Activation layer (no gradients)\")\n",
    "            layer_idx += 1\n",
    "\n",
    "# Single training step that mimics your original code\n",
    "def single_training_step(step_idx, current_state, simulation_states, trainable_params, opt_state):\n",
    "    \n",
    "    print(f\"\\n=== STEP {step_idx} ===\")\n",
    "    \n",
    "    def evolve_loss_fn(network_params_current):\n",
    "        # Use the passed parameters directly\n",
    "        \n",
    "        # Reconstruct full model\n",
    "        full_model = eqx.combine(network_params_current, static_params)\n",
    "        \n",
    "        # Simulate one step\n",
    "        new_state = mock_simulation_step(current_state, full_model, step_idx)\n",
    "        \n",
    "        # Update simulation data storage\n",
    "        updated_states = simulation_states.at[step_idx].set(new_state)\n",
    "        \n",
    "        # Compute loss using data_lag approach (like your original code)\n",
    "        end_idx = step_idx + 1\n",
    "        start_idx = jnp.maximum(0, end_idx - data_lag)\n",
    "        actual_length = end_idx - start_idx\n",
    "        \n",
    "        # Get predicted and ground truth states\n",
    "        predicted_states = jax.lax.dynamic_slice_in_dim(\n",
    "            updated_states, start_idx, data_lag, axis=0\n",
    "        )\n",
    "        ground_truth_states = jax.lax.dynamic_slice_in_dim(\n",
    "            target_data, start_idx, data_lag, axis=0\n",
    "        )\n",
    "        \n",
    "        # Handle masking for early steps\n",
    "        if actual_length < data_lag:\n",
    "            mask = jnp.arange(data_lag) < actual_length\n",
    "            expanded_mask = mask.reshape(-1, *([1] * (predicted_states.ndim - 1)))\n",
    "            masked_pred = predicted_states * expanded_mask\n",
    "            masked_gt = ground_truth_states * expanded_mask\n",
    "            loss = mock_loss_function(masked_pred, masked_gt, None)\n",
    "            print(f\"  Using masked loss (length {actual_length}/{data_lag})\")\n",
    "        else:\n",
    "            loss = mock_loss_function(predicted_states, ground_truth_states, None)\n",
    "            print(f\"  Using full loss (length {data_lag})\")\n",
    "        \n",
    "        # Debug info (avoid printing traced values during gradient computation)\n",
    "        # These will only print during the sensitivity test, not during grad computation\n",
    "        state_norm = jnp.linalg.norm(new_state)\n",
    "        correction_effect = jnp.linalg.norm(new_state - current_state)\n",
    "        pred_norm = jnp.linalg.norm(predicted_states)\n",
    "        gt_norm = jnp.linalg.norm(ground_truth_states)\n",
    "        \n",
    "        # Update state for next iteration (return new values)\n",
    "        return loss, new_state, updated_states, state_norm, correction_effect, pred_norm, gt_norm\n",
    "    \n",
    "    # Test loss sensitivity to parameters\n",
    "    print(\"Testing parameter sensitivity:\")\n",
    "    original_loss, current_state, simulation_states, state_norm, correction_effect, pred_norm, gt_norm = evolve_loss_fn(trainable_params)\n",
    "    \n",
    "    # Print debug info from sensitivity test\n",
    "    print(f\"  State norm: {state_norm:.6f}\")\n",
    "    print(f\"  Correction effect: {correction_effect:.6f}\")\n",
    "    print(f\"  Predicted norm: {pred_norm:.6f}\")\n",
    "    print(f\"  Target norm: {gt_norm:.6f}\")\n",
    "    print(f\"  Loss value: {original_loss:.8f}\")\n",
    "    \n",
    "    # Reset state for perturbed test\n",
    "    temp_state = current_state\n",
    "    temp_sim_states = simulation_states\n",
    "    \n",
    "    # Perturb parameters slightly\n",
    "    perturbed_params = jax.tree.map(\n",
    "        lambda x: x + 0.001 * jax.random.normal(jax.random.PRNGKey(step_idx), x.shape), \n",
    "        trainable_params\n",
    "    )\n",
    "    perturbed_loss, _, _, _, _, _, _ = evolve_loss_fn(perturbed_params)\n",
    "    \n",
    "    # Restore state\n",
    "    current_state = temp_state\n",
    "    simulation_states = temp_sim_states\n",
    "    \n",
    "    print(f\"  Original loss: {original_loss:.8f}\")\n",
    "    print(f\"  Perturbed loss: {perturbed_loss:.8f}\")\n",
    "    print(f\"  Sensitivity: {abs(perturbed_loss - original_loss):.8f}\")\n",
    "    \n",
    "    if abs(perturbed_loss - original_loss) < 1e-12:\n",
    "        print(\"  WARNING: Loss is not sensitive to parameter changes!\")\n",
    "    \n",
    "    # Compute gradients (need a function that only returns loss)\n",
    "    print(\"Computing gradients...\")\n",
    "    def loss_only_fn(network_params_current):\n",
    "        loss, _, _, _, _, _, _ = evolve_loss_fn(network_params_current)\n",
    "        return loss\n",
    "        \n",
    "    loss_value, grads = eqx.filter_value_and_grad(loss_only_fn)(trainable_params)\n",
    "    \n",
    "    # Print gradient information\n",
    "    print_gradient_norms(grads, f\"Step {step_idx}\")\n",
    "    \n",
    "    # Check if gradients are zero\n",
    "    total_grad_norm = jax.tree.reduce(\n",
    "        lambda acc, x: acc + jnp.sum(x**2) if hasattr(x, 'shape') else acc, \n",
    "        grads, 0.0\n",
    "    )\n",
    "    print(f\"  Total gradient norm: {jnp.sqrt(total_grad_norm):.8f}\")\n",
    "    \n",
    "    if jnp.sqrt(total_grad_norm) < 1e-10:\n",
    "        print(\"  WARNING: Gradients are essentially zero!\")\n",
    "    \n",
    "    # Update parameters\n",
    "    updates, new_opt_state = optimizer.update(grads, opt_state, trainable_params)\n",
    "    new_trainable_params = eqx.apply_updates(trainable_params, updates)\n",
    "    \n",
    "    # Check parameter change\n",
    "    param_diffs = jax.tree.map(lambda x, y: jnp.sum((x - y)**2) if hasattr(x, 'shape') else 0.0, \n",
    "                               trainable_params, new_trainable_params)\n",
    "    param_change = jax.tree.reduce(lambda acc, x: acc + x, param_diffs, 0.0)\n",
    "    print(f\"  Parameter change norm: {jnp.sqrt(param_change):.8f}\")\n",
    "    \n",
    "    # Update global state (return new values)\n",
    "    return loss_value, total_grad_norm, new_trainable_params, new_opt_state, current_state, simulation_states\n",
    "\n",
    "# Run one training step\n",
    "print(\"Running single training step debug...\")\n",
    "loss, grad_norm, trainable_params, opt_state, current_state, simulation_states = single_training_step(\n",
    "    0, current_state, simulation_states, trainable_params, opt_state\n",
    ")\n",
    "\n",
    "print(f\"\\nFinal Summary:\")\n",
    "print(f\"Loss: {loss:.8f}\")\n",
    "print(f\"Total gradient norm: {jnp.sqrt(grad_norm):.8f}\")\n",
    "\n",
    "# Test a few more steps to see evolution\n",
    "for step in range(1, min(3, total_steps)):\n",
    "    loss, grad_norm, trainable_params, opt_state, current_state, simulation_states = single_training_step(\n",
    "        step, current_state, simulation_states, trainable_params, opt_state\n",
    "    )\n",
    "    print(f\"Step {step} - Loss: {loss:.8f}, Grad norm: {jnp.sqrt(grad_norm):.8f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91bc2052",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "thesis_2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
